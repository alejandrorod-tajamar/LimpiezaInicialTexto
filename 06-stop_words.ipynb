{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ada93fc",
   "metadata": {},
   "source": [
    "**Práctica: Eliminación Contextual de Stopwords para Análisis de Reseñas de Productos**  \n",
    "\n",
    "**Contexto:** Eres un NLP Engineer en **ReviewBoost**, una startup que analiza reseñas de Amazon para identificar problemas críticos en productos.  \n",
    "\n",
    "---\n",
    "\n",
    "### **Fase 1: Análisis Inicial**  \n",
    "**Objetivo:** Entender cómo las stopwords por defecto afectan el significado en reseñas.  \n",
    "\n",
    "#### **Tarea 1:**  \n",
    "- **Descargar** el dataset `comensarios_clintes.csv`. Puedes utilizar otro dataset de internet.  \n",
    "- **Ejemplo de Entrada:**  \n",
    "```No recomiendo este producto. Aunque es barato, se rompió en dos días.\n",
    "``` \n",
    "- **Procesar** el texto usando spaCy con stopwords por defecto.  \n",
    "- **Registrar:**  \n",
    "  - ¿Qué palabras clave se eliminaron (ej. \"no\", \"aunque\")?  \n",
    "  - ¿Cómo afecta esto al significado?  \n",
    "\n",
    "**Procesamiento Actual (spaCy):**  \n",
    "```  \n",
    "[\"mal\", \"producto\", \"entrega\", \"pésima\", \"marketminddecepciona\"]  \n",
    "```  \n",
    "**Problema:** La palabra \"no\" se eliminó, invirtiendo el significado.  \n",
    "\n",
    "#### **Pistas:**  \n",
    "- Usar `spacy.load(\"es_core_news_sm\")` y `token.is_stop`.  \n",
    "- Para debuggear: Imprimir lista de stopwords con `print(nlp.Defaults.stop_words)`.  \n",
    "\n",
    "#### **Verificación:**  \n",
    "El estudiante debe generar una tabla con 5 ejemplos donde la eliminación de stopwords alteró el significado.  \n",
    " \n",
    "\n",
    "---\n",
    "\n",
    "### **Fase 2: Personalización de la Lista**  \n",
    "**Objetivo:** Crear una lista de stopwords adaptada a reseñas de productos.  \n",
    "\n",
    "#### **Tarea 2:**  \n",
    "1. **Preservar Términos Clave:**  \n",
    "   - Negaciones: \"no\", \"nunca\", \"tampoco\".  \n",
    "   - Conectores de contraste: \"pero\", \"aunque\", \"sin embargo\".  \n",
    "2. **Eliminar Términos Genéricos:**  \n",
    "   - Palabras redundantes: \"producto\", \"cliente\", \"día\".  \n",
    "   - Verbos comunes sin contexto: \"hacer\", \"tener\", \"decir\".  \n",
    "3. **Añadir Stopwords Específicas:**  \n",
    "   - Términos no informativos: \"hola\", \"gracias\", \"pd\".  \n",
    "\n",
    "#### **Pasos:**  \n",
    "- **Analizar Frecuencia:** Usar `Counter` de Python para identificar palabras repetidas en el 90% de las reseñas.  \n",
    "- **Modificar Lista:**  \n",
    "  - Cargar stopwords de spaCy.  \n",
    "  - Quitar términos críticos (ej. `stopwords_es.discard(\"no\")`).  \n",
    "  - Añadir términos redundantes (ej. `stopwords_es.add(\"producto\")`).  \n",
    "\n",
    "#### **Pistas:**  \n",
    "- Para \"hola\" y \"gracias\", usar regex: `r'\\b(hola|gracias)\\b'`.  \n",
    "- Usar `nltk.corpus.stopwords.words('spanish')` como lista alternativa si hay inconsistencias.  \n",
    "\n",
    "---\n",
    "\n",
    "### **Fase 3: Implementación y Pruebas**  \n",
    "**Objetivo:** Validar que la lista personalizada preserva el contexto crítico.  \n",
    "\n",
    "#### **Tarea 3:**  \n",
    "1. **Función de Procesamiento:**  \n",
    "   - Input: Texto crudo.  \n",
    "   - Output: Lista de tokens sin stopwords personalizadas.  \n",
    "2. **Casos de Prueba:**  \n",
    "   ```  \n",
    "   Texto 1: \"No funciona bien, pero el diseño es bonito.\"  \n",
    "   Output Esperado: [\"no\", \"funciona\", \"bien\", \"pero\", \"diseño\", \"bonito\"]  \n",
    "\n",
    "   Texto 2: \"Nunca compré algo tan malo. Aunque el precio es bajo, no lo vale.\"  \n",
    "   Output Esperado: [\"nunca\", \"compré\", \"malo\", \"aunque\", \"precio\", \"bajo\", \"no\", \"vale\"]  \n",
    "   ```  \n",
    "3. **Métricas:**  \n",
    "   - Precisión: 100% de los términos clave preservados en 20 casos de prueba predefinidos.  \n",
    "\n",
    "#### **Pistas:**  \n",
    "- Usar `token.text.lower()` para normalizar.  \n",
    "- Si \"aunque\" se elimina, revisar `stopwords_es.remove(\"aunque\")`.  \n",
    "\n",
    "---\n",
    "\n",
    "### **Fase 4: Evaluación de Impacto**  \n",
    "**Objetivo:** Medir cómo afecta la personalización al análisis de sentimiento.  \n",
    "\n",
    "#### **Tarea 4:**  \n",
    "1. **Entrenar Modelo Básico:**  \n",
    "   - Usar `sklearn` (CountVectorizer + LogisticRegression).  \n",
    "   - Comparar resultados con/sin stopwords personalizadas.  \n",
    "2. **Métricas:**  \n",
    "   - Exactitud (accuracy) en un subset de 200 reseñas etiquetadas manualmente.  \n",
    "3. **Análisis:**  \n",
    "   - ¿En qué reseñas mejoró/deterioró la clasificación?  \n",
    "\n",
    "#### **Pistas:**  \n",
    "- Usar `max_features=1000` en CountVectorizer para reducir dimensionalidad.  \n",
    "- Ejemplo de mejora: Reseñas con negaciones clasificadas correctamente.  \n",
    "\n",
    "---\n",
    "\n",
    "### **Entrega Final**  \n",
    "1. **Código:**  \n",
    "   - Script `stopwords_custom.py` (funciones de carga, procesamiento, y evaluación).  \n",
    "2. **Documentación:**  \n",
    "   - Lista final de stopwords (`.txt`).  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3906d83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importación de librerías necesarias\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import re\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Cargar el modelo de spaCy en español\n",
    "try:\n",
    "    nlp = spacy.load(\"es_core_news_sm\")\n",
    "except OSError:\n",
    "    print(\"Descargando modelo de spaCy para español...\")\n",
    "    !python -m spacy download es_core_news_sm\n",
    "    nlp = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "# Configuración para visualizaciones\n",
    "plt.style.use('ggplot')\n",
    "sns.set(style='whitegrid')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09d3ec67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset cargado correctamente. Shape: (50, 2)\n",
      "\n",
      "Primeras 5 filas:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>texto</th>\n",
       "      <th>etiqueta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>El paquete llegó con el embalaje comprometido,...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Después de 3 semanas de espera, el artículo nu...</td>\n",
       "      <td>negativo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>La atención al cliente fue evasiva y poco reso...</td>\n",
       "      <td>negativo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Entrega express y producto en perfecto estado....</td>\n",
       "      <td>positivo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>El sistema de seguimiento en línea mostró info...</td>\n",
       "      <td>negativo</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               texto  etiqueta\n",
       "0  El paquete llegó con el embalaje comprometido,...   neutral\n",
       "1  Después de 3 semanas de espera, el artículo nu...  negativo\n",
       "2  La atención al cliente fue evasiva y poco reso...  negativo\n",
       "3  Entrega express y producto en perfecto estado....  positivo\n",
       "4  El sistema de seguimiento en línea mostró info...  negativo"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Distribución de etiquetas:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "etiqueta\n",
       "negativo    25\n",
       "positivo    19\n",
       "neutral      6\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cargar el dataset de comentarios\n",
    "try:\n",
    "    df = pd.read_csv('comentarios_clientes.csv')\n",
    "    print(f\"Dataset cargado correctamente. Shape: {df.shape}\")\n",
    "    print(\"\\nPrimeras 5 filas:\")\n",
    "    display(df.head())\n",
    "    \n",
    "    print(\"\\nDistribución de etiquetas:\")\n",
    "    display(df['etiqueta'].value_counts())\n",
    "except Exception as e:\n",
    "    print(f\"Error al cargar el dataset: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ac6cba",
   "metadata": {},
   "source": [
    "## Implementación Fase 1: Análisis Inicial\n",
    "A continuación, analizaremos cómo las stopwords por defecto afectan el significado de las reseñas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95215291",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de stopwords en spaCy (español): 521\n",
      "Muestra de 15 stopwords: ['menos', 'tu', 'ahí', 'nuevo', 'indicó', 'vosotras', 'último', 'cualquier', 'según', 'muchas', 'ninguno', 'se', 'esta', 'demasiado', 'lado']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ejemplo</th>\n",
       "      <th>Texto Original</th>\n",
       "      <th>Sin Stopwords (Default)</th>\n",
       "      <th>Palabras Clave Eliminadas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>No recomiendo este producto. Aunque es barato,...</td>\n",
       "      <td>[recomiendo, producto, barato, rompió]</td>\n",
       "      <td>[no, este, aunque, es, se, en, dos, días]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>El producto nunca funcionó correctamente, pero...</td>\n",
       "      <td>[producto, funcionó, correctamente, servicio, ...</td>\n",
       "      <td>[el, nunca, pero, el, al, fue]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Aunque el embalaje estaba dañado, el producto ...</td>\n",
       "      <td>[embalaje, dañado, producto, condiciones]</td>\n",
       "      <td>[aunque, el, estaba, el, llegó, en, buenas]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>No me gustó la calidad, sin embargo el precio ...</td>\n",
       "      <td>[gustó, calidad, precio]</td>\n",
       "      <td>[no, me, la, sin, embargo, el, es, muy, bueno]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Nunca había visto algo tan malo como este artí...</td>\n",
       "      <td>[visto, malo, artículo]</td>\n",
       "      <td>[nunca, había, algo, tan, como, este]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Ejemplo                                     Texto Original  \\\n",
       "0        1  No recomiendo este producto. Aunque es barato,...   \n",
       "1        2  El producto nunca funcionó correctamente, pero...   \n",
       "2        3  Aunque el embalaje estaba dañado, el producto ...   \n",
       "3        4  No me gustó la calidad, sin embargo el precio ...   \n",
       "4        5  Nunca había visto algo tan malo como este artí...   \n",
       "\n",
       "                             Sin Stopwords (Default)  \\\n",
       "0             [recomiendo, producto, barato, rompió]   \n",
       "1  [producto, funcionó, correctamente, servicio, ...   \n",
       "2          [embalaje, dañado, producto, condiciones]   \n",
       "3                           [gustó, calidad, precio]   \n",
       "4                            [visto, malo, artículo]   \n",
       "\n",
       "                        Palabras Clave Eliminadas  \n",
       "0       [no, este, aunque, es, se, en, dos, días]  \n",
       "1                  [el, nunca, pero, el, al, fue]  \n",
       "2     [aunque, el, estaba, el, llegó, en, buenas]  \n",
       "3  [no, me, la, sin, embargo, el, es, muy, bueno]  \n",
       "4           [nunca, había, algo, tan, como, este]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Análisis manual del impacto en el significado:\n",
      "\n",
      "Ejemplo 1: No recomiendo este producto. Aunque es barato, se rompió en dos días.\n",
      "Palabras clave eliminadas: ['no', 'este', 'aunque', 'es', 'se', 'en', 'dos', 'días']\n",
      "Describir cómo esto afecta el significado: [COMPLETAR MANUALMENTE]\n",
      "\n",
      "Ejemplo 2: El producto nunca funcionó correctamente, pero el servicio al cliente fue excelente.\n",
      "Palabras clave eliminadas: ['el', 'nunca', 'pero', 'el', 'al', 'fue']\n",
      "Describir cómo esto afecta el significado: [COMPLETAR MANUALMENTE]\n",
      "\n",
      "Ejemplo 3: Aunque el embalaje estaba dañado, el producto llegó en buenas condiciones.\n",
      "Palabras clave eliminadas: ['aunque', 'el', 'estaba', 'el', 'llegó', 'en', 'buenas']\n",
      "Describir cómo esto afecta el significado: [COMPLETAR MANUALMENTE]\n",
      "\n",
      "Ejemplo 4: No me gustó la calidad, sin embargo el precio es muy bueno.\n",
      "Palabras clave eliminadas: ['no', 'me', 'la', 'sin', 'embargo', 'el', 'es', 'muy', 'bueno']\n",
      "Describir cómo esto afecta el significado: [COMPLETAR MANUALMENTE]\n",
      "\n",
      "Ejemplo 5: Nunca había visto algo tan malo como este artículo.\n",
      "Palabras clave eliminadas: ['nunca', 'había', 'algo', 'tan', 'como', 'este']\n",
      "Describir cómo esto afecta el significado: [COMPLETAR MANUALMENTE]\n"
     ]
    }
   ],
   "source": [
    "# Fase 1: Función para procesar texto con stopwords por defecto de spaCy\n",
    "def procesar_texto_default(texto):\n",
    "    \"\"\"Procesa texto usando spaCy con las stopwords por defecto.\"\"\"\n",
    "    doc = nlp(texto)\n",
    "    # Extraer tokens que no son stopwords y no son signos de puntuación\n",
    "    tokens = [token.text.lower() for token in doc if not token.is_stop and not token.is_punct]\n",
    "    return tokens\n",
    "\n",
    "# Función para procesar texto pero conservando stopwords (para comparación)\n",
    "def procesar_texto_con_stopwords(texto):\n",
    "    \"\"\"Procesa texto usando spaCy pero conservando stopwords.\"\"\"\n",
    "    doc = nlp(texto)\n",
    "    # Extraer tokens que no son signos de puntuación\n",
    "    tokens = [token.text.lower() for token in doc if not token.is_punct]\n",
    "    return tokens\n",
    "\n",
    "# Ver la lista de stopwords en español de spaCy\n",
    "print(\"Total de stopwords en spaCy (español):\", len(nlp.Defaults.stop_words))\n",
    "print(\"Muestra de 15 stopwords:\", list(nlp.Defaults.stop_words)[:15])\n",
    "\n",
    "# Analizar ejemplos para ver el impacto de eliminar stopwords\n",
    "ejemplos = [\n",
    "    \"No recomiendo este producto. Aunque es barato, se rompió en dos días.\",\n",
    "    \"El producto nunca funcionó correctamente, pero el servicio al cliente fue excelente.\",\n",
    "    \"Aunque el embalaje estaba dañado, el producto llegó en buenas condiciones.\",\n",
    "    \"No me gustó la calidad, sin embargo el precio es muy bueno.\",\n",
    "    \"Nunca había visto algo tan malo como este artículo.\"  \n",
    "]\n",
    "\n",
    "# Crear tabla comparativa\n",
    "resultados = []\n",
    "for i, ejemplo in enumerate(ejemplos, 1):\n",
    "    con_sw = procesar_texto_con_stopwords(ejemplo)\n",
    "    sin_sw = procesar_texto_default(ejemplo)\n",
    "    \n",
    "    # Identificar palabras clave eliminadas\n",
    "    eliminadas = [w for w in con_sw if w not in sin_sw and w in nlp.Defaults.stop_words]\n",
    "    \n",
    "    resultados.append({\n",
    "        \"Ejemplo\": i,\n",
    "        \"Texto Original\": ejemplo,\n",
    "        \"Con Stopwords\": con_sw,\n",
    "        \"Sin Stopwords (Default)\": sin_sw,\n",
    "        \"Palabras Clave Eliminadas\": eliminadas,\n",
    "        \"Impacto en Significado\": \"Analizar manualmente\"\n",
    "    })\n",
    "\n",
    "# Mostrar tabla de resultados\n",
    "df_resultados = pd.DataFrame(resultados)\n",
    "display(df_resultados[['Ejemplo', 'Texto Original', 'Sin Stopwords (Default)', 'Palabras Clave Eliminadas']])\n",
    "\n",
    "# Información para análisis manual\n",
    "print(\"\\nAnálisis manual del impacto en el significado:\")\n",
    "for i, ejemplo in enumerate(ejemplos, 1):\n",
    "    print(f\"\\nEjemplo {i}: {ejemplo}\")\n",
    "    print(f\"Palabras clave eliminadas: {resultados[i-1]['Palabras Clave Eliminadas']}\")\n",
    "    print(\"Describir cómo esto afecta el significado: [COMPLETAR MANUALMENTE]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03be752",
   "metadata": {},
   "source": [
    "## Implementación Fase 2: Personalización de la Lista de Stopwords\n",
    "A continuación, crearemos una lista personalizada de stopwords adaptada al análisis de reseñas de productos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d9ceb6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Palabras más comunes en el dataset:\n",
      "[('de', 39), ('en', 22), ('con', 14), ('el', 10), ('del', 8), ('la', 7), ('para', 6), ('a', 5), ('productos', 5), ('artículo', 4), ('y', 4), ('seguimiento', 3), ('mi', 3), ('producto', 3), ('estado', 3), ('sistema', 3), ('proceso', 3), ('pero', 3), ('que', 3), ('pago', 3)]\n",
      "Término preservado: 'no'\n",
      "Término preservado: 'nunca'\n",
      "Término preservado: 'tampoco'\n",
      "Término preservado: 'pero'\n",
      "Término preservado: 'aunque'\n",
      "Término preservado: 'sin'\n",
      "Término preservado: 'embargo'\n",
      "Término añadido: 'producto'\n",
      "Término añadido: 'cliente'\n",
      "Término añadido: 'día'\n",
      "Término añadido: 'hacer'\n",
      "Término añadido: 'tener'\n",
      "Término añadido: 'decir'\n",
      "Término añadido: 'hola'\n",
      "Término añadido: 'gracias'\n",
      "Término añadido: 'pd'\n",
      "\n",
      "Palabras frecuentes que podrían añadirse como stopwords:\n",
      "['con', 'del', 'para', 'productos', 'artículo', 'seguimiento', 'producto', 'estado', 'sistema', 'proceso', 'que', 'pago', 'programa', 'opciones', 'problemas']\n",
      "Añadida palabra frecuente: 'con'\n",
      "Añadida palabra frecuente: 'del'\n",
      "Añadida palabra frecuente: 'para'\n",
      "Añadida palabra frecuente: 'productos'\n",
      "Añadida palabra frecuente: 'artículo'\n",
      "Añadida palabra frecuente: 'seguimiento'\n",
      "Añadida palabra frecuente: 'producto'\n",
      "Añadida palabra frecuente: 'estado'\n",
      "Añadida palabra frecuente: 'sistema'\n",
      "Añadida palabra frecuente: 'proceso'\n",
      "\n",
      "Total stopwords originales: 521\n",
      "Total stopwords personalizadas: 524\n",
      "Lista de stopwords personalizadas guardada en 'stopwords_personalizadas.txt'\n"
     ]
    }
   ],
   "source": [
    "# Fase 2: Personalización de stopwords\n",
    "\n",
    "# 1. Analizar frecuencia de palabras en el dataset\n",
    "def analizar_frecuencia(df, columna_texto):\n",
    "    \"\"\"Analiza la frecuencia de palabras en un dataset.\"\"\"\n",
    "    # Concatenar todos los textos\n",
    "    texto_completo = ' '.join(df[columna_texto].astype(str).values)\n",
    "    # Procesar con spaCy\n",
    "    doc = nlp(texto_completo)\n",
    "    # Crear contador de palabras (excluyendo puntuación)\n",
    "    palabras = [token.text.lower() for token in doc if not token.is_punct]\n",
    "    contador = Counter(palabras)\n",
    "    return contador\n",
    "\n",
    "# Obtener frecuencias\n",
    "frequencias = analizar_frecuencia(df, 'texto')\n",
    "print(\"Palabras más comunes en el dataset:\")\n",
    "print(frequencias.most_common(20))\n",
    "\n",
    "# 2. Personalizar lista de stopwords\n",
    "\n",
    "# Obtener lista base de stopwords\n",
    "stopwords_personalizadas = set(nlp.Defaults.stop_words)\n",
    "\n",
    "# Términos clave a preservar (eliminar de la lista de stopwords)\n",
    "terminos_a_preservar = [\n",
    "    \"no\", \"nunca\", \"tampoco\",  # Negaciones\n",
    "    \"pero\", \"aunque\", \"sin\", \"embargo\"  # Conectores de contraste\n",
    "]\n",
    "\n",
    "# Términos genéricos a eliminar (añadir a la lista de stopwords)\n",
    "terminos_genericos = [\n",
    "    \"producto\", \"cliente\", \"día\",  # Palabras redundantes\n",
    "    \"hacer\", \"tener\", \"decir\",     # Verbos comunes sin contexto\n",
    "    \"hola\", \"gracias\", \"pd\"       # Términos no informativos\n",
    "]\n",
    "\n",
    "# Modificar la lista de stopwords\n",
    "for termino in terminos_a_preservar:\n",
    "    if termino in stopwords_personalizadas:\n",
    "        stopwords_personalizadas.remove(termino)\n",
    "        print(f\"Término preservado: '{termino}'\")\n",
    "\n",
    "for termino in terminos_genericos:\n",
    "    stopwords_personalizadas.add(termino)\n",
    "    print(f\"Término añadido: '{termino}'\")\n",
    "\n",
    "# 3. Palabras muy frecuentes en el dataset que podrían ser stopwords\n",
    "palabras_frecuentes = [palabra for palabra, freq in frequencias.most_common(30) \n",
    "                      if len(palabra) > 2 and palabra not in terminos_a_preservar]\n",
    "\n",
    "print(\"\\nPalabras frecuentes que podrían añadirse como stopwords:\")\n",
    "print(palabras_frecuentes[:15])\n",
    "\n",
    "# Añadir palabras muy frecuentes (con cuidado)\n",
    "for palabra in palabras_frecuentes[:10]:\n",
    "    if palabra not in terminos_a_preservar and len(palabra) > 2:\n",
    "        stopwords_personalizadas.add(palabra)\n",
    "        print(f\"Añadida palabra frecuente: '{palabra}'\")\n",
    "\n",
    "# Mostrar tamaño de las listas de stopwords\n",
    "print(f\"\\nTotal stopwords originales: {len(nlp.Defaults.stop_words)}\")\n",
    "print(f\"Total stopwords personalizadas: {len(stopwords_personalizadas)}\")\n",
    "\n",
    "# Guardar la lista personalizada de stopwords\n",
    "with open('stopwords_personalizadas.txt', 'w', encoding='utf-8') as f:\n",
    "    for word in sorted(stopwords_personalizadas):\n",
    "        f.write(word + '\\n')\n",
    "\n",
    "print(\"Lista de stopwords personalizadas guardada en 'stopwords_personalizadas.txt'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c91188f",
   "metadata": {},
   "source": [
    "## Implementación Fase 3: Implementación y Pruebas\n",
    "A continuación, implementaremos la función de procesamiento utilizando nuestra lista personalizada de stopwords y la probaremos con casos específicos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c95fede",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Caso</th>\n",
       "      <th>Texto</th>\n",
       "      <th>Resultado</th>\n",
       "      <th>Esperado</th>\n",
       "      <th>¿Coincide?</th>\n",
       "      <th>Precisión (%)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>No funciona bien, pero el diseño es bonito.</td>\n",
       "      <td>[no, funciona, pero, diseño, bonito]</td>\n",
       "      <td>[no, funciona, bien, pero, diseño, bonito]</td>\n",
       "      <td>False</td>\n",
       "      <td>83.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Nunca compré algo tan malo. Aunque el precio e...</td>\n",
       "      <td>[nunca, compré, malo, aunque, precio, no, vale]</td>\n",
       "      <td>[nunca, compré, malo, aunque, precio, bajo, no...</td>\n",
       "      <td>False</td>\n",
       "      <td>87.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>El paquete llegó con el embalaje comprometido,...</td>\n",
       "      <td>[paquete, embalaje, comprometido, aunque, logr...</td>\n",
       "      <td>[paquete, llegó, embalaje, comprometido, aunqu...</td>\n",
       "      <td>False</td>\n",
       "      <td>87.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>No me gustó para nada la calidad del material,...</td>\n",
       "      <td>[no, gustó, calidad, material, pero, precio, j...</td>\n",
       "      <td>[no, gustó, nada, calidad, material, pero, pre...</td>\n",
       "      <td>False</td>\n",
       "      <td>87.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Nunca había visto un servicio tan malo como este.</td>\n",
       "      <td>[nunca, visto, servicio, malo]</td>\n",
       "      <td>[nunca, visto, servicio, tan, malo]</td>\n",
       "      <td>False</td>\n",
       "      <td>80.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Caso                                              Texto  \\\n",
       "0     1        No funciona bien, pero el diseño es bonito.   \n",
       "1     2  Nunca compré algo tan malo. Aunque el precio e...   \n",
       "2     3  El paquete llegó con el embalaje comprometido,...   \n",
       "3     4  No me gustó para nada la calidad del material,...   \n",
       "4     5  Nunca había visto un servicio tan malo como este.   \n",
       "\n",
       "                                           Resultado  \\\n",
       "0               [no, funciona, pero, diseño, bonito]   \n",
       "1    [nunca, compré, malo, aunque, precio, no, vale]   \n",
       "2  [paquete, embalaje, comprometido, aunque, logr...   \n",
       "3  [no, gustó, calidad, material, pero, precio, j...   \n",
       "4                     [nunca, visto, servicio, malo]   \n",
       "\n",
       "                                            Esperado  ¿Coincide?  \\\n",
       "0         [no, funciona, bien, pero, diseño, bonito]       False   \n",
       "1  [nunca, compré, malo, aunque, precio, bajo, no...       False   \n",
       "2  [paquete, llegó, embalaje, comprometido, aunqu...       False   \n",
       "3  [no, gustó, nada, calidad, material, pero, pre...       False   \n",
       "4                [nunca, visto, servicio, tan, malo]       False   \n",
       "\n",
       "   Precisión (%)  \n",
       "0      83.333333  \n",
       "1      87.500000  \n",
       "2      87.500000  \n",
       "3      87.500000  \n",
       "4      80.000000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Precisión promedio: 85.17%\n",
      "\n",
      "Casos con problemas que requieren ajuste:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Caso</th>\n",
       "      <th>Texto</th>\n",
       "      <th>Términos faltantes</th>\n",
       "      <th>Términos extra</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>No funciona bien, pero el diseño es bonito.</td>\n",
       "      <td>[bien]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Nunca compré algo tan malo. Aunque el precio e...</td>\n",
       "      <td>[bajo]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>El paquete llegó con el embalaje comprometido,...</td>\n",
       "      <td>[llegó]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>No me gustó para nada la calidad del material,...</td>\n",
       "      <td>[nada]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Nunca había visto un servicio tan malo como este.</td>\n",
       "      <td>[tan]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Caso                                              Texto Términos faltantes  \\\n",
       "0     1        No funciona bien, pero el diseño es bonito.             [bien]   \n",
       "1     2  Nunca compré algo tan malo. Aunque el precio e...             [bajo]   \n",
       "2     3  El paquete llegó con el embalaje comprometido,...            [llegó]   \n",
       "3     4  No me gustó para nada la calidad del material,...             [nada]   \n",
       "4     5  Nunca había visto un servicio tan malo como este.              [tan]   \n",
       "\n",
       "  Términos extra  \n",
       "0             []  \n",
       "1             []  \n",
       "2             []  \n",
       "3             []  \n",
       "4             []  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ajustando lista de stopwords...\n",
      "Quitando 'bien' de las stopwords\n",
      "Quitando 'bajo' de las stopwords\n",
      "Quitando 'llegó' de las stopwords\n",
      "Quitando 'nada' de las stopwords\n",
      "Quitando 'tan' de las stopwords\n"
     ]
    }
   ],
   "source": [
    "# Fase 3: Implementación y pruebas\n",
    "\n",
    "# 1. Función de procesamiento con stopwords personalizadas\n",
    "def procesar_texto_personalizado(texto, stopwords_personalizadas):\n",
    "    \"\"\"Procesa texto eliminando stopwords personalizadas.\n",
    "    \n",
    "    Args:\n",
    "        texto (str): Texto a procesar\n",
    "        stopwords_personalizadas (set): Conjunto de stopwords personalizadas\n",
    "        \n",
    "    Returns:\n",
    "        list: Lista de tokens sin stopwords personalizadas\n",
    "    \"\"\"\n",
    "    doc = nlp(texto)\n",
    "    # Extraer tokens que no son stopwords personalizadas y no son signos de puntuación\n",
    "    tokens = [token.text.lower() for token in doc \n",
    "              if token.text.lower() not in stopwords_personalizadas \n",
    "              and not token.is_punct]\n",
    "    return tokens\n",
    "\n",
    "# 2. Casos de prueba\n",
    "casos_prueba = [\n",
    "    {\n",
    "        \"texto\": \"No funciona bien, pero el diseño es bonito.\",\n",
    "        \"esperado\": [\"no\", \"funciona\", \"bien\", \"pero\", \"diseño\", \"bonito\"]\n",
    "    },\n",
    "    {\n",
    "        \"texto\": \"Nunca compré algo tan malo. Aunque el precio es bajo, no lo vale.\",\n",
    "        \"esperado\": [\"nunca\", \"compré\", \"malo\", \"aunque\", \"precio\", \"bajo\", \"no\", \"vale\"]\n",
    "    },\n",
    "    {\n",
    "        \"texto\": \"El paquete llegó con el embalaje comprometido, aunque lograron reembolsarme rápidamente.\",\n",
    "        \"esperado\": [\"paquete\", \"llegó\", \"embalaje\", \"comprometido\", \"aunque\", \"lograron\", \"reembolsarme\", \"rápidamente\"]\n",
    "    },\n",
    "    {\n",
    "        \"texto\": \"No me gustó para nada la calidad del material, pero el precio es justo.\",\n",
    "        \"esperado\": [\"no\", \"gustó\", \"nada\", \"calidad\", \"material\", \"pero\", \"precio\", \"justo\"]\n",
    "    },\n",
    "    {\n",
    "        \"texto\": \"Nunca había visto un servicio tan malo como este.\",\n",
    "        \"esperado\": [\"nunca\", \"visto\", \"servicio\", \"tan\", \"malo\"]\n",
    "    }\n",
    "]\n",
    "\n",
    "# 3. Evaluación de los casos de prueba\n",
    "resultados_prueba = []\n",
    "for i, caso in enumerate(casos_prueba, 1):\n",
    "    resultado = procesar_texto_personalizado(caso[\"texto\"], stopwords_personalizadas)\n",
    "    \n",
    "    # Comparar con el resultado esperado\n",
    "    terminos_faltantes = [t for t in caso[\"esperado\"] if t not in resultado]\n",
    "    terminos_extra = [t for t in resultado if t not in caso[\"esperado\"]]\n",
    "    coincide = (terminos_faltantes == [] and terminos_extra == [])\n",
    "    precision = len(set(caso[\"esperado\"]) & set(resultado)) / len(set(caso[\"esperado\"])) * 100\n",
    "    \n",
    "    resultados_prueba.append({\n",
    "        \"Caso\": i,\n",
    "        \"Texto\": caso[\"texto\"],\n",
    "        \"Resultado\": resultado,\n",
    "        \"Esperado\": caso[\"esperado\"],\n",
    "        \"Términos faltantes\": terminos_faltantes,\n",
    "        \"Términos extra\": terminos_extra,\n",
    "        \"¿Coincide?\": coincide,\n",
    "        \"Precisión (%)\": precision\n",
    "    })\n",
    "    \n",
    "# Mostrar resultados\n",
    "df_pruebas = pd.DataFrame(resultados_prueba)\n",
    "display(df_pruebas[['Caso', 'Texto', 'Resultado', 'Esperado', '¿Coincide?', 'Precisión (%)']])\n",
    "\n",
    "# Resumen de precisión\n",
    "precision_total = sum(df_pruebas['Precisión (%)']) / len(df_pruebas)\n",
    "print(f\"\\nPrecisión promedio: {precision_total:.2f}%\")\n",
    "\n",
    "# Ajuste fino: Si hay términos problemáticos, ajustar la lista\n",
    "problemas = df_pruebas[df_pruebas['Precisión (%)'] < 100]\n",
    "if not problemas.empty:\n",
    "    print(\"\\nCasos con problemas que requieren ajuste:\")\n",
    "    display(problemas[['Caso', 'Texto', 'Términos faltantes', 'Términos extra']])\n",
    "    \n",
    "    # Ajustar la lista de stopwords si es necesario\n",
    "    print(\"\\nAjustando lista de stopwords...\")\n",
    "    for _, caso in problemas.iterrows():\n",
    "        for termino in caso['Términos faltantes']:\n",
    "            if termino in stopwords_personalizadas:\n",
    "                stopwords_personalizadas.remove(termino)\n",
    "                print(f\"Quitando '{termino}' de las stopwords\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2c1753",
   "metadata": {},
   "source": [
    "## Implementación Fase 4: Evaluación de Impacto\n",
    "A continuación, evaluaremos el impacto de nuestra lista personalizada de stopwords en el análisis de sentimiento de las reseñas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8bcc3e26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exactitud con stopwords por defecto: 0.6000\n",
      "Exactitud con stopwords personalizadas: 0.4667\n",
      "Mejora: -13.33%\n",
      "\n",
      "Clasificación con stopwords por defecto:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negativo       0.54      1.00      0.70         7\n",
      "     neutral       0.00      0.00      0.00         2\n",
      "    positivo       1.00      0.33      0.50         6\n",
      "\n",
      "    accuracy                           0.60        15\n",
      "   macro avg       0.51      0.44      0.40        15\n",
      "weighted avg       0.65      0.60      0.53        15\n",
      "\n",
      "\n",
      "Clasificación con stopwords personalizadas:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negativo       0.47      1.00      0.64         7\n",
      "     neutral       0.00      0.00      0.00         2\n",
      "    positivo       0.00      0.00      0.00         6\n",
      "\n",
      "    accuracy                           0.47        15\n",
      "   macro avg       0.16      0.33      0.21        15\n",
      "weighted avg       0.22      0.47      0.30        15\n",
      "\n",
      "\n",
      "Encontrados 2 ejemplos donde las predicciones difieren:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/aleja/Desktop/Master/05 - IA Generativa/Desarrollo/13-NLP/LimpiezaInicialTexto/.venvWSL/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/mnt/c/Users/aleja/Desktop/Master/05 - IA Generativa/Desarrollo/13-NLP/LimpiezaInicialTexto/.venvWSL/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/mnt/c/Users/aleja/Desktop/Master/05 - IA Generativa/Desarrollo/13-NLP/LimpiezaInicialTexto/.venvWSL/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/mnt/c/Users/aleja/Desktop/Master/05 - IA Generativa/Desarrollo/13-NLP/LimpiezaInicialTexto/.venvWSL/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/mnt/c/Users/aleja/Desktop/Master/05 - IA Generativa/Desarrollo/13-NLP/LimpiezaInicialTexto/.venvWSL/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/mnt/c/Users/aleja/Desktop/Master/05 - IA Generativa/Desarrollo/13-NLP/LimpiezaInicialTexto/.venvWSL/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Texto</th>\n",
       "      <th>Etiqueta Real</th>\n",
       "      <th>Predicción Default</th>\n",
       "      <th>Predicción Custom</th>\n",
       "      <th>Custom Correcta</th>\n",
       "      <th>Default Correcta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Experiencia realidad aumentada para visualizar...</td>\n",
       "      <td>positivo</td>\n",
       "      <td>positivo</td>\n",
       "      <td>negativo</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Programa de pruebas gratuitas para productos d...</td>\n",
       "      <td>positivo</td>\n",
       "      <td>positivo</td>\n",
       "      <td>negativo</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Texto Etiqueta Real  \\\n",
       "0  Experiencia realidad aumentada para visualizar...      positivo   \n",
       "1  Programa de pruebas gratuitas para productos d...      positivo   \n",
       "\n",
       "  Predicción Default Predicción Custom  Custom Correcta  Default Correcta  \n",
       "0           positivo          negativo            False              True  \n",
       "1           positivo          negativo            False              True  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El modelo con stopwords personalizadas acertó en 0 de estos casos.\n",
      "El modelo con stopwords por defecto acertó en 2 de estos casos.\n"
     ]
    }
   ],
   "source": [
    "# Fase 4: Evaluación de Impacto en Análisis de Sentimiento\n",
    "\n",
    "# Preparar los datos para entrenamiento\n",
    "# Nos aseguramos que el dataset tiene al menos las columnas 'texto' y 'etiqueta'\n",
    "assert 'texto' in df.columns and 'etiqueta' in df.columns, \"El dataset debe tener columnas 'texto' y 'etiqueta'\"\n",
    "\n",
    "# Preparar función para preprocesamiento\n",
    "def preprocesar_dataset(df, columna_texto, usar_stopwords_personalizadas=False):\n",
    "    \"\"\"Preprocesa los textos del dataset.\n",
    "    \n",
    "    Args:\n",
    "        df (DataFrame): DataFrame con los textos\n",
    "        columna_texto (str): Nombre de la columna con los textos\n",
    "        usar_stopwords_personalizadas (bool): Si True, usa las stopwords personalizadas\n",
    "        \n",
    "    Returns:\n",
    "        list: Lista de textos procesados\n",
    "    \"\"\"\n",
    "    textos_procesados = []\n",
    "    for texto in df[columna_texto]:\n",
    "        if usar_stopwords_personalizadas:\n",
    "            tokens = procesar_texto_personalizado(texto, stopwords_personalizadas)\n",
    "        else:\n",
    "            tokens = procesar_texto_default(texto)\n",
    "        textos_procesados.append(' '.join(tokens))\n",
    "    return textos_procesados\n",
    "\n",
    "# Dividir datos en entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df['texto'], df['etiqueta'], test_size=0.3, random_state=42, stratify=df['etiqueta'])\n",
    "\n",
    "# Preprocesar los textos\n",
    "# 1. Usando stopwords por defecto\n",
    "X_train_default = preprocesar_dataset(pd.DataFrame({'texto': X_train}), 'texto', False)\n",
    "X_test_default = preprocesar_dataset(pd.DataFrame({'texto': X_test}), 'texto', False)\n",
    "\n",
    "# 2. Usando stopwords personalizadas\n",
    "X_train_custom = preprocesar_dataset(pd.DataFrame({'texto': X_train}), 'texto', True)\n",
    "X_test_custom = preprocesar_dataset(pd.DataFrame({'texto': X_test}), 'texto', True)\n",
    "\n",
    "# Vectorizar los textos\n",
    "vectorizer_default = CountVectorizer(max_features=1000)\n",
    "vectorizer_custom = CountVectorizer(max_features=1000)\n",
    "\n",
    "# Vectorización con stopwords por defecto\n",
    "X_train_default_vec = vectorizer_default.fit_transform(X_train_default)\n",
    "X_test_default_vec = vectorizer_default.transform(X_test_default)\n",
    "\n",
    "# Vectorización con stopwords personalizadas\n",
    "X_train_custom_vec = vectorizer_custom.fit_transform(X_train_custom)\n",
    "X_test_custom_vec = vectorizer_custom.transform(X_test_custom)\n",
    "\n",
    "# Entrenar modelo con stopwords por defecto\n",
    "model_default = LogisticRegression(max_iter=1000, random_state=42)\n",
    "model_default.fit(X_train_default_vec, y_train)\n",
    "\n",
    "# Entrenar modelo con stopwords personalizadas\n",
    "model_custom = LogisticRegression(max_iter=1000, random_state=42)\n",
    "model_custom.fit(X_train_custom_vec, y_train)\n",
    "\n",
    "# Evaluar modelos\n",
    "y_pred_default = model_default.predict(X_test_default_vec)\n",
    "y_pred_custom = model_custom.predict(X_test_custom_vec)\n",
    "\n",
    "# Calcular métricas\n",
    "accuracy_default = accuracy_score(y_test, y_pred_default)\n",
    "accuracy_custom = accuracy_score(y_test, y_pred_custom)\n",
    "\n",
    "print(f\"Exactitud con stopwords por defecto: {accuracy_default:.4f}\")\n",
    "print(f\"Exactitud con stopwords personalizadas: {accuracy_custom:.4f}\")\n",
    "print(f\"Mejora: {(accuracy_custom - accuracy_default)*100:.2f}%\")\n",
    "\n",
    "# Reportes detallados\n",
    "print(\"\\nClasificación con stopwords por defecto:\")\n",
    "print(classification_report(y_test, y_pred_default))\n",
    "\n",
    "print(\"\\nClasificación con stopwords personalizadas:\")\n",
    "print(classification_report(y_test, y_pred_custom))\n",
    "\n",
    "# Análisis de ejemplos donde difieren las predicciones\n",
    "resultados_comparativos = []\n",
    "for i, (texto, etiqueta_real) in enumerate(zip(X_test, y_test)):\n",
    "    pred_default = y_pred_default[i]\n",
    "    pred_custom = y_pred_custom[i]\n",
    "    \n",
    "    if pred_default != pred_custom:\n",
    "        resultados_comparativos.append({\n",
    "            \"Texto\": texto,\n",
    "            \"Etiqueta Real\": etiqueta_real,\n",
    "            \"Predicción Default\": pred_default,\n",
    "            \"Predicción Custom\": pred_custom,\n",
    "            \"Custom Correcta\": pred_custom == etiqueta_real,\n",
    "            \"Default Correcta\": pred_default == etiqueta_real\n",
    "        })\n",
    "\n",
    "# Mostrar ejemplos donde difieren las predicciones\n",
    "if resultados_comparativos:\n",
    "    df_comparativa = pd.DataFrame(resultados_comparativos)\n",
    "    print(f\"\\nEncontrados {len(df_comparativa)} ejemplos donde las predicciones difieren:\")\n",
    "    display(df_comparativa)\n",
    "    \n",
    "    # Contar cuántas veces cada modelo acertó en estos casos\n",
    "    mejora_custom = df_comparativa[\"Custom Correcta\"].sum()\n",
    "    mejora_default = df_comparativa[\"Default Correcta\"].sum()\n",
    "    print(f\"El modelo con stopwords personalizadas acertó en {mejora_custom} de estos casos.\")\n",
    "    print(f\"El modelo con stopwords por defecto acertó en {mejora_default} de estos casos.\")\n",
    "else:\n",
    "    print(\"\\nNo se encontraron diferencias en las predicciones de ambos modelos.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa70b5bd",
   "metadata": {},
   "source": [
    "## Implementación Final: Creación del script stopwords_custom.py\n",
    "A continuación, crearemos el script `stopwords_custom.py` que contendrá las funciones desarrolladas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90ca4ab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Script stopwords_custom.py creado exitosamente.\n",
      "Error al importar el módulo: unterminated string literal (detected at line 123) (stopwords_custom.py, line 123)\n"
     ]
    }
   ],
   "source": [
    "# Crear el script stopwords_custom.py\n",
    "\n",
    "# Definir contenido del script\n",
    "script_content = '''\n",
    "\"\"\"Módulo para la gestión personalizada de stopwords en análisis de reseñas.\n",
    "\n",
    "Este módulo contiene funciones para cargar, personalizar y aplicar stopwords\n",
    "en el contexto de análisis de reseñas de productos, preservando términos\n",
    "críticos como negaciones y conectores de contraste.\n",
    "\"\"\"\n",
    "\n",
    "import spacy\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "# Cargar modelo de spaCy\n",
    "try:\n",
    "    nlp = spacy.load(\"es_core_news_sm\")\n",
    "except OSError:\n",
    "    print(\"El modelo de spaCy para español no está instalado.\")\n",
    "    print(\"Instálalo con: python -m spacy download es_core_news_sm\")\n",
    "    raise\n",
    "\n",
    "\n",
    "def cargar_stopwords_personalizadas(ruta_archivo=None):\n",
    "    \"\"\"Carga la lista personalizada de stopwords.\n",
    "    \n",
    "    Args:\n",
    "        ruta_archivo (str, optional): Ruta al archivo con stopwords personalizadas.\n",
    "            Si es None, retorna la lista predefinida en este módulo.\n",
    "    \n",
    "    Returns:\n",
    "        set: Conjunto de stopwords personalizadas\n",
    "    \"\"\"\n",
    "    if ruta_archivo:\n",
    "        try:\n",
    "            with open(ruta_archivo, \"r\", encoding=\"utf-8\") as f:\n",
    "                return set(line.strip() for line in f)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Archivo {ruta_archivo} no encontrado. Usando lista predefinida.\")\n",
    "    \n",
    "    # Lista predefinida (stopwords de spaCy con modificaciones)\n",
    "    stopwords_personalizadas = set(nlp.Defaults.stop_words)\n",
    "    \n",
    "    # Términos clave a preservar\n",
    "    terminos_a_preservar = [\n",
    "        \"no\", \"nunca\", \"tampoco\",  # Negaciones\n",
    "        \"pero\", \"aunque\", \"sin\", \"embargo\"  # Conectores de contraste\n",
    "    ]\n",
    "    \n",
    "    # Términos genéricos a eliminar\n",
    "    terminos_genericos = [\n",
    "        \"producto\", \"cliente\", \"día\",  # Palabras redundantes\n",
    "        \"hacer\", \"tener\", \"decir\",     # Verbos comunes sin contexto\n",
    "        \"hola\", \"gracias\", \"pd\"       # Términos no informativos\n",
    "    ]\n",
    "    \n",
    "    # Modificar la lista\n",
    "    for termino in terminos_a_preservar:\n",
    "        if termino in stopwords_personalizadas:\n",
    "            stopwords_personalizadas.remove(termino)\n",
    "            \n",
    "    for termino in terminos_genericos:\n",
    "        stopwords_personalizadas.add(termino)\n",
    "        \n",
    "    return stopwords_personalizadas\n",
    "\n",
    "\n",
    "def procesar_texto(texto, stopwords=None, conservar_stopwords=False):\n",
    "    \"\"\"Procesa texto eliminando o conservando stopwords.\n",
    "    \n",
    "    Args:\n",
    "        texto (str): Texto a procesar\n",
    "        stopwords (set, optional): Conjunto de stopwords a utilizar.\n",
    "            Si es None, usa las stopwords por defecto de spaCy.\n",
    "        conservar_stopwords (bool): Si es True, conserva todas las stopwords.\n",
    "    \n",
    "    Returns:\n",
    "        list: Lista de tokens procesados\n",
    "    \"\"\"\n",
    "    doc = nlp(texto)\n",
    "    \n",
    "    if conservar_stopwords:\n",
    "        return [token.text.lower() for token in doc if not token.is_punct]\n",
    "    \n",
    "    if stopwords is None:\n",
    "        # Usar stopwords por defecto de spaCy\n",
    "        return [token.text.lower() for token in doc \n",
    "                if not token.is_stop and not token.is_punct]\n",
    "    else:\n",
    "        # Usar lista personalizada de stopwords\n",
    "        return [token.text.lower() for token in doc \n",
    "                if token.text.lower() not in stopwords and not token.is_punct]\n",
    "\n",
    "\n",
    "def analizar_frecuencia(textos):\n",
    "    \"\"\"Analiza la frecuencia de palabras en un conjunto de textos.\n",
    "    \n",
    "    Args:\n",
    "        textos (list or Series): Lista o Series de textos a analizar\n",
    "    \n",
    "    Returns:\n",
    "        Counter: Contador con las frecuencias de palabras\n",
    "    \"\"\"\n",
    "    if isinstance(textos, pd.Series):\n",
    "        textos = textos.astype(str).values\n",
    "        \n",
    "    # Concatenar todos los textos\n",
    "    texto_completo = \" \".join(textos)\n",
    "    doc = nlp(texto_completo)\n",
    "    \n",
    "    # Contar palabras (excluyendo puntuación)\n",
    "    palabras = [token.text.lower() for token in doc if not token.is_punct]\n",
    "    return Counter(palabras)\n",
    "\n",
    "\n",
    "def guardar_stopwords(stopwords, ruta_archivo):\n",
    "    \"\"\"Guarda la lista de stopwords en un archivo.\n",
    "    \n",
    "    Args:\n",
    "        stopwords (set): Conjunto de stopwords a guardar\n",
    "        ruta_archivo (str): Ruta donde guardar el archivo\n",
    "    \"\"\"\n",
    "    with open(ruta_archivo, \"w\", encoding=\"utf-8\") as f:\n",
    "        for word in sorted(stopwords):\n",
    "            f.write(word + \"\\n\")\n",
    "    print(f\"Lista de stopwords guardada en {ruta_archivo}\")\n",
    "\n",
    "\n",
    "def evaluar_precision(casos_prueba, stopwords):\n",
    "    \"\"\"Evalúa la precisión de la lista de stopwords en casos de prueba.\n",
    "    \n",
    "    Args:\n",
    "        casos_prueba (list): Lista de diccionarios con casos de prueba.\n",
    "            Cada diccionario debe tener las claves \"texto\" y \"esperado\".\n",
    "        stopwords (set): Conjunto de stopwords a evaluar\n",
    "    \n",
    "    Returns:\n",
    "        float: Precisión promedio (0-100)\n",
    "    \"\"\"\n",
    "    resultados = []\n",
    "    for caso in casos_prueba:\n",
    "        resultado = procesar_texto(caso[\"texto\"], stopwords)\n",
    "        precision = len(set(caso[\"esperado\"]) & set(resultado)) / len(set(caso[\"esperado\"])) * 100\n",
    "        resultados.append(precision)\n",
    "    \n",
    "    return sum(resultados) / len(resultados)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Ejemplo de uso\n",
    "    print(\"Módulo de procesamiento de stopwords personalizadas\")\n",
    "    print(\"Ejemplo de uso:\")\n",
    "    \n",
    "    texto_ejemplo = \"No recomiendo este producto. Aunque es barato, se rompió en dos días.\"\n",
    "    \n",
    "    # Procesamiento con stopwords por defecto\n",
    "    print(\"\\nProcesamiento con stopwords por defecto:\")\n",
    "    tokens_default = procesar_texto(texto_ejemplo)\n",
    "    print(tokens_default)\n",
    "    \n",
    "    # Procesamiento con stopwords personalizadas\n",
    "    print(\"\\nProcesamiento con stopwords personalizadas:\")\n",
    "    stopwords_custom = cargar_stopwords_personalizadas()\n",
    "    tokens_custom = procesar_texto(texto_ejemplo, stopwords_custom)\n",
    "    print(tokens_custom)\n",
    "'''\n",
    "\n",
    "# Guardar el script\n",
    "with open('stopwords_custom.py', 'w', encoding='utf-8') as f:\n",
    "    f.write(script_content)\n",
    "\n",
    "print(\"Script stopwords_custom.py creado exitosamente.\")\n",
    "\n",
    "# Verificar que se puede importar\n",
    "try:\n",
    "    from stopwords_custom import cargar_stopwords_personalizadas, procesar_texto\n",
    "    print(\"El módulo se importó correctamente.\")\n",
    "    \n",
    "    # Probar el módulo\n",
    "    sw = cargar_stopwords_personalizadas()\n",
    "    ejemplo = \"No recomiendo este producto. Aunque es barato, se rompió en dos días.\"\n",
    "    resultado = procesar_texto(ejemplo, sw)\n",
    "    print(f\"\\nProcesamiento con módulo importado:\\n{resultado}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error al importar el módulo: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venvWSL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
